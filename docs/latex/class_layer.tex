\hypertarget{class_layer}{}\section{Layer$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$ Class Template Reference}
\label{class_layer}\index{Layer$<$ L\+E\+N\+\_\+\+I\+N, L\+E\+N\+\_\+\+O\+U\+T $>$@{Layer$<$ L\+E\+N\+\_\+\+I\+N, L\+E\+N\+\_\+\+O\+U\+T $>$}}


{\ttfamily \#include $<$Layer.\+h$>$}

\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\hyperlink{class_layer_a458cd765beb3e884d631d15e0e6cb690}{Layer} (int \hyperlink{class_layer_a844e22c542ae02cb475cabb8353300a8}{len\+In}, int \hyperlink{class_layer_ae10a61035ba7a18f9f2a6d1d3ebf9811}{len\+Out}, \hyperlink{class_a_f_activation_function}{A\+F\+Activation\+Function}$<$ double, L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$activation\+Fn)
\item 
\hyperlink{class_layer_a3ed1ecdffa6a2bb492aa282f029e85ae}{$\sim$\+Layer} ()
\item 
void \hyperlink{class_layer_aee349ae2e2c54fa8079cc564e84cb938}{randomize\+Weights} ()
\item 
void \hyperlink{class_layer_a5088f4edabd0de4d59f3c59cc8190786}{forward\+Pass} (array$<$ double, L\+E\+N\+\_\+\+IN $>$ $\ast$\hyperlink{class_layer_a38145ae44adb77f559a0ae77dad7c6f9}{input\+Vals}, array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$\hyperlink{class_layer_a5007d0043790b288b9473ec233108482}{output\+Vals})
\item 
void \hyperlink{class_layer_a5f09327f0ab9f2d5b0d8fbe3e7994714}{forward\+Pass} (array$<$ double, L\+E\+N\+\_\+\+IN $>$ $\ast$\hyperlink{class_layer_a38145ae44adb77f559a0ae77dad7c6f9}{input\+Vals})
\item 
{\footnotesize template$<$size\+\_\+t L\+E\+N\+\_\+\+O\+U\+T\+\_\+\+N\+E\+XT$>$ }\\void \hyperlink{class_layer_a00f3e4879c6074dab5dd095b52791e88}{backpropagate} (array$<$ double, L\+E\+N\+\_\+\+IN $>$ $\ast$next\+Deltas, \hyperlink{class_a_f_matrix}{A\+F\+Matrix}$<$ double, L\+E\+N\+\_\+\+O\+U\+T\+\_\+\+N\+E\+XT, L\+E\+N\+\_\+\+IN $>$ $\ast$next\+Weights, array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$new\+Deltas)
\item 
{\footnotesize template$<$size\+\_\+t L\+E\+N\+\_\+\+O\+U\+T\+\_\+\+N\+E\+XT$>$ }\\void \hyperlink{class_layer_ac31f77202649fa3cefbeb1242075adf7}{backpropagate} (array$<$ double, L\+E\+N\+\_\+\+IN $>$ $\ast$next\+Deltas, \hyperlink{class_a_f_matrix}{A\+F\+Matrix}$<$ double, L\+E\+N\+\_\+\+O\+U\+T\+\_\+\+N\+E\+XT, L\+E\+N\+\_\+\+IN $>$ $\ast$next\+Weights)
\item 
void \hyperlink{class_layer_ac2b3a1c895cc243730809fc3fc68868b}{backpropagate\+Base} (array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$actual\+Vals, array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$expected\+Vals, \hyperlink{class_a_f_loss_function}{A\+F\+Loss\+Function} $\ast$loss\+Fn, array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$new\+Deltas)
\item 
void \hyperlink{class_layer_a24abcd1a8327a079ec32a663b228b399}{backpropagate\+Base} (array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$actual\+Vals, array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$expected\+Vals, \hyperlink{class_a_f_loss_function}{A\+F\+Loss\+Function} $\ast$loss\+Fn)
\item 
void \hyperlink{class_layer_aa490ca327cae4e71d017d444c2c87468}{update\+Weights} ()
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
int \hyperlink{class_layer_a844e22c542ae02cb475cabb8353300a8}{len\+In}
\begin{DoxyCompactList}\small\item\em The size of the vector that this layer takes as input. \end{DoxyCompactList}\item 
int \hyperlink{class_layer_ae10a61035ba7a18f9f2a6d1d3ebf9811}{len\+Out}
\begin{DoxyCompactList}\small\item\em The size of the vector that this layer outputs. \end{DoxyCompactList}\item 
array$<$ double, L\+E\+N\+\_\+\+IN $>$ $\ast$ \hyperlink{class_layer_a38145ae44adb77f559a0ae77dad7c6f9}{input\+Vals}
\begin{DoxyCompactList}\small\item\em The values that this layer receives from the previous layer. \end{DoxyCompactList}\item 
array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$ \hyperlink{class_layer_aa0a3384dec407f34bf9d93994fc6613f}{sums}
\begin{DoxyCompactList}\small\item\em The sums after the weights are multiplied by input value`. \end{DoxyCompactList}\item 
array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$ \hyperlink{class_layer_aeefbb274f205f4960c4a1a3ed5f5d807}{deltas}
\begin{DoxyCompactList}\small\item\em The intermediate gradients of the loss, {\ttfamily deltas\mbox{[}i\mbox{]} = d(\+Error)/d(sum\+\_\+i)} \end{DoxyCompactList}\item 
array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$ \hyperlink{class_layer_a5007d0043790b288b9473ec233108482}{output\+Vals}
\begin{DoxyCompactList}\small\item\em The values after the sums are put through the activation function. \end{DoxyCompactList}\item 
\hyperlink{class_a_f_matrix}{A\+F\+Matrix}$<$ double, L\+E\+N\+\_\+\+O\+UT, L\+E\+N\+\_\+\+IN $>$ $\ast$ \hyperlink{class_layer_aa362ee5edaf5b0fe8d5d7c4674ded7a1}{weights}
\begin{DoxyCompactList}\small\item\em The weights which are multiplied against the input values. This has {\ttfamily len\+Out} rows and {\ttfamily len\+In} cols. \end{DoxyCompactList}\item 
\hyperlink{class_a_f_matrix}{A\+F\+Matrix}$<$ double, L\+E\+N\+\_\+\+O\+UT, L\+E\+N\+\_\+\+IN $>$ $\ast$ \hyperlink{class_layer_ab4afc6b6fdda2b5fd21621f40be776fd}{weight\+Gradient}
\begin{DoxyCompactList}\small\item\em The weight gradients. {\ttfamily weight\+Gradient\mbox{[}i,j\mbox{]} = d(\+Error)/d(weights\mbox{[}i,j\mbox{]})}. Same shape as {\ttfamily weights}. \end{DoxyCompactList}\item 
\hyperlink{class_a_f_activation_function}{A\+F\+Activation\+Function}$<$ double, L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$ \hyperlink{class_layer_aa986af731d3638fe75d0fd93e3481386}{activation\+Function}
\begin{DoxyCompactList}\small\item\em The activation function {\ttfamily g} such that `output\+Values = g(weights $\ast$ Input\+Vals). Note that g takes a vector. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$\newline
class Layer$<$ L\+E\+N\+\_\+\+I\+N, L\+E\+N\+\_\+\+O\+U\+T $>$}

\begin{DoxyAuthor}{Author}
Aryan Falahatpisheh 
\end{DoxyAuthor}


\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{class_layer_a458cd765beb3e884d631d15e0e6cb690}\label{class_layer_a458cd765beb3e884d631d15e0e6cb690}} 
\index{Layer@{Layer}!Layer@{Layer}}
\index{Layer@{Layer}!Layer@{Layer}}
\subsubsection{\texorpdfstring{Layer()}{Layer()}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
\hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::\hyperlink{class_layer}{Layer} (\begin{DoxyParamCaption}\item[{int}]{len\+In,  }\item[{int}]{len\+Out,  }\item[{\hyperlink{class_a_f_activation_function}{A\+F\+Activation\+Function}$<$ double, L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$}]{activation\+Fn }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}


\begin{DoxyParams}{Parameters}
{\em len\+In} & The input length of this layer \\
\hline
{\em len\+Out} & The output size of this layer \\
\hline
{\em activation\+Fn} & Pass an \hyperlink{class_a_f_activation_function}{A\+F\+Activation\+Function} by value so this layer knows how to calculate output values. \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{class_layer_a3ed1ecdffa6a2bb492aa282f029e85ae}\label{class_layer_a3ed1ecdffa6a2bb492aa282f029e85ae}} 
\index{Layer@{Layer}!````~Layer@{$\sim$\+Layer}}
\index{````~Layer@{$\sim$\+Layer}!Layer@{Layer}}
\subsubsection{\texorpdfstring{$\sim$\+Layer()}{~Layer()}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
\hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::$\sim$\hyperlink{class_layer}{Layer} (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



\subsection{Member Function Documentation}
\mbox{\Hypertarget{class_layer_a00f3e4879c6074dab5dd095b52791e88}\label{class_layer_a00f3e4879c6074dab5dd095b52791e88}} 
\index{Layer@{Layer}!backpropagate@{backpropagate}}
\index{backpropagate@{backpropagate}!Layer@{Layer}}
\subsubsection{\texorpdfstring{backpropagate()}{backpropagate()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
template$<$size\+\_\+t L\+E\+N\+\_\+\+O\+U\+T\+\_\+\+N\+E\+XT$>$ \\
void \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::backpropagate (\begin{DoxyParamCaption}\item[{array$<$ double, L\+E\+N\+\_\+\+IN $>$ $\ast$}]{next\+Deltas,  }\item[{\hyperlink{class_a_f_matrix}{A\+F\+Matrix}$<$ double, L\+E\+N\+\_\+\+O\+U\+T\+\_\+\+N\+E\+XT, L\+E\+N\+\_\+\+IN $>$ $\ast$}]{next\+Weights,  }\item[{array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$}]{new\+Deltas }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}

Performs backpropogation algorithm. Writes this layer\textquotesingle{}s new d(\+Err)/d(Sums) into {\ttfamily new\+Deltas}. 
\begin{DoxyTemplParams}{Template Parameters}
{\em L\+E\+N\+\_\+\+O\+U\+T\+\_\+\+N\+E\+XT} & The next layer\textquotesingle{}s output length \\
\hline
\end{DoxyTemplParams}

\begin{DoxyParams}{Parameters}
{\em next\+Deltas} & The next layer\textquotesingle{}s d(\+Err)/d(sums); \\
\hline
{\em next\+Weights} & \\
\hline
{\em new\+Deltas} & \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{class_layer_ac31f77202649fa3cefbeb1242075adf7}\label{class_layer_ac31f77202649fa3cefbeb1242075adf7}} 
\index{Layer@{Layer}!backpropagate@{backpropagate}}
\index{backpropagate@{backpropagate}!Layer@{Layer}}
\subsubsection{\texorpdfstring{backpropagate()}{backpropagate()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
template$<$size\+\_\+t L\+E\+N\+\_\+\+O\+U\+T\+\_\+\+N\+E\+XT$>$ \\
void \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::backpropagate (\begin{DoxyParamCaption}\item[{array$<$ double, L\+E\+N\+\_\+\+IN $>$ $\ast$}]{next\+Deltas,  }\item[{\hyperlink{class_a_f_matrix}{A\+F\+Matrix}$<$ double, L\+E\+N\+\_\+\+O\+U\+T\+\_\+\+N\+E\+XT, L\+E\+N\+\_\+\+IN $>$ $\ast$}]{next\+Weights }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}

Performs backpropogation algorithm and writes output to {\ttfamily this-\/$>$deltas}. 
\begin{DoxyTemplParams}{Template Parameters}
{\em L\+E\+N\+\_\+\+O\+U\+T\+\_\+\+N\+E\+XT} & \\
\hline
\end{DoxyTemplParams}

\begin{DoxyParams}{Parameters}
{\em next\+Deltas} & \\
\hline
{\em next\+Weights} & \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{class_layer_ac2b3a1c895cc243730809fc3fc68868b}\label{class_layer_ac2b3a1c895cc243730809fc3fc68868b}} 
\index{Layer@{Layer}!backpropagate\+Base@{backpropagate\+Base}}
\index{backpropagate\+Base@{backpropagate\+Base}!Layer@{Layer}}
\subsubsection{\texorpdfstring{backpropagate\+Base()}{backpropagateBase()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
void \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::backpropagate\+Base (\begin{DoxyParamCaption}\item[{array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$}]{actual\+Vals,  }\item[{array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$}]{expected\+Vals,  }\item[{\hyperlink{class_a_f_loss_function}{A\+F\+Loss\+Function} $\ast$}]{loss\+Fn,  }\item[{array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$}]{new\+Deltas }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}

The backprop algorithm for the last layer. First calculates {\ttfamily d(\+Err)/d(output\+Vals)}, which is the derivative of the loss function w.\+r.\+t to {\ttfamily actual\+Vals}. It then calculates d(\+Err)/d(sums)`.


\begin{DoxyParams}{Parameters}
{\em actual\+Vals} & \\
\hline
{\em expected\+Vals} & \\
\hline
{\em new\+Deltas} & \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{class_layer_a24abcd1a8327a079ec32a663b228b399}\label{class_layer_a24abcd1a8327a079ec32a663b228b399}} 
\index{Layer@{Layer}!backpropagate\+Base@{backpropagate\+Base}}
\index{backpropagate\+Base@{backpropagate\+Base}!Layer@{Layer}}
\subsubsection{\texorpdfstring{backpropagate\+Base()}{backpropagateBase()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
void \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::backpropagate\+Base (\begin{DoxyParamCaption}\item[{array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$}]{actual\+Vals,  }\item[{array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$}]{expected\+Vals,  }\item[{\hyperlink{class_a_f_loss_function}{A\+F\+Loss\+Function} $\ast$}]{loss\+Fn }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}

The backprop algorithm for the last layer. First calculates {\ttfamily d(\+Err)/d(output\+Vals)}, which is the derivative of the loss function w.\+r.\+t to {\ttfamily actual\+Vals}. It then calculates d(\+Err)/d(sums)`.


\begin{DoxyParams}{Parameters}
{\em actual\+Vals} & \\
\hline
{\em expected\+Vals} & \\
\hline
{\em new\+Deltas} & \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{class_layer_a5088f4edabd0de4d59f3c59cc8190786}\label{class_layer_a5088f4edabd0de4d59f3c59cc8190786}} 
\index{Layer@{Layer}!forward\+Pass@{forward\+Pass}}
\index{forward\+Pass@{forward\+Pass}!Layer@{Layer}}
\subsubsection{\texorpdfstring{forward\+Pass()}{forwardPass()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
void \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::forward\+Pass (\begin{DoxyParamCaption}\item[{array$<$ double, L\+E\+N\+\_\+\+IN $>$ $\ast$}]{input\+Vals,  }\item[{array$<$ double, L\+E\+N\+\_\+\+O\+UT $>$ $\ast$}]{output\+Vals }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}

Will perform the forward pass on this \hyperlink{class_layer}{Layer}. Will take in {\ttfamily input\+Vals}, calculate weighted sums, and then pass that result to this layer\textquotesingle{}s activation function. The output will be written to {\ttfamily output\+Vals}. 
\begin{DoxyParams}{Parameters}
{\em input\+Vals} & \\
\hline
{\em output\+Vals} & -\/ output\+Vals\mbox{[}i\mbox{]} = this-\/$>$weights.\+row(i).inner\+Product(input\+Vals). \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{class_layer_a5f09327f0ab9f2d5b0d8fbe3e7994714}\label{class_layer_a5f09327f0ab9f2d5b0d8fbe3e7994714}} 
\index{Layer@{Layer}!forward\+Pass@{forward\+Pass}}
\index{forward\+Pass@{forward\+Pass}!Layer@{Layer}}
\subsubsection{\texorpdfstring{forward\+Pass()}{forwardPass()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
void \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::forward\+Pass (\begin{DoxyParamCaption}\item[{array$<$ double, L\+E\+N\+\_\+\+IN $>$ $\ast$}]{input\+Vals }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}

Will perform the forward pass on this \hyperlink{class_layer}{Layer}. Will take in {\ttfamily input\+Vals}, calculate weighted sums, and then pass that result to this layer\textquotesingle{}s activation function. The output will be written to {\ttfamily output\+Vals}. 
\begin{DoxyParams}{Parameters}
{\em input\+Vals} & \\
\hline
{\em output\+Vals} & -\/ output\+Vals\mbox{[}i\mbox{]} = this-\/$>$weights.\+row(i).inner\+Product(input\+Vals). \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{class_layer_aee349ae2e2c54fa8079cc564e84cb938}\label{class_layer_aee349ae2e2c54fa8079cc564e84cb938}} 
\index{Layer@{Layer}!randomize\+Weights@{randomize\+Weights}}
\index{randomize\+Weights@{randomize\+Weights}!Layer@{Layer}}
\subsubsection{\texorpdfstring{randomize\+Weights()}{randomizeWeights()}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
void \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::randomize\+Weights (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}

\mbox{\Hypertarget{class_layer_aa490ca327cae4e71d017d444c2c87468}\label{class_layer_aa490ca327cae4e71d017d444c2c87468}} 
\index{Layer@{Layer}!update\+Weights@{update\+Weights}}
\index{update\+Weights@{update\+Weights}!Layer@{Layer}}
\subsubsection{\texorpdfstring{update\+Weights()}{updateWeights()}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
void \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::update\+Weights (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



\subsection{Member Data Documentation}
\mbox{\Hypertarget{class_layer_aa986af731d3638fe75d0fd93e3481386}\label{class_layer_aa986af731d3638fe75d0fd93e3481386}} 
\index{Layer@{Layer}!activation\+Function@{activation\+Function}}
\index{activation\+Function@{activation\+Function}!Layer@{Layer}}
\subsubsection{\texorpdfstring{activation\+Function}{activationFunction}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
\hyperlink{class_a_f_activation_function}{A\+F\+Activation\+Function}$<$double, L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT$>$$\ast$ \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::activation\+Function}



The activation function {\ttfamily g} such that `output\+Values = g(weights $\ast$ Input\+Vals). Note that g takes a vector. 

\mbox{\Hypertarget{class_layer_aeefbb274f205f4960c4a1a3ed5f5d807}\label{class_layer_aeefbb274f205f4960c4a1a3ed5f5d807}} 
\index{Layer@{Layer}!deltas@{deltas}}
\index{deltas@{deltas}!Layer@{Layer}}
\subsubsection{\texorpdfstring{deltas}{deltas}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
array$<$double, L\+E\+N\+\_\+\+O\+UT$>$$\ast$ \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::deltas}



The intermediate gradients of the loss, {\ttfamily deltas\mbox{[}i\mbox{]} = d(\+Error)/d(sum\+\_\+i)} 

\mbox{\Hypertarget{class_layer_a38145ae44adb77f559a0ae77dad7c6f9}\label{class_layer_a38145ae44adb77f559a0ae77dad7c6f9}} 
\index{Layer@{Layer}!input\+Vals@{input\+Vals}}
\index{input\+Vals@{input\+Vals}!Layer@{Layer}}
\subsubsection{\texorpdfstring{input\+Vals}{inputVals}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
array$<$double, L\+E\+N\+\_\+\+IN$>$$\ast$ \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::input\+Vals}



The values that this layer receives from the previous layer. 

\mbox{\Hypertarget{class_layer_a844e22c542ae02cb475cabb8353300a8}\label{class_layer_a844e22c542ae02cb475cabb8353300a8}} 
\index{Layer@{Layer}!len\+In@{len\+In}}
\index{len\+In@{len\+In}!Layer@{Layer}}
\subsubsection{\texorpdfstring{len\+In}{lenIn}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
int \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::len\+In}



The size of the vector that this layer takes as input. 

Hello! \mbox{\Hypertarget{class_layer_ae10a61035ba7a18f9f2a6d1d3ebf9811}\label{class_layer_ae10a61035ba7a18f9f2a6d1d3ebf9811}} 
\index{Layer@{Layer}!len\+Out@{len\+Out}}
\index{len\+Out@{len\+Out}!Layer@{Layer}}
\subsubsection{\texorpdfstring{len\+Out}{lenOut}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
int \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::len\+Out}



The size of the vector that this layer outputs. 

\mbox{\Hypertarget{class_layer_a5007d0043790b288b9473ec233108482}\label{class_layer_a5007d0043790b288b9473ec233108482}} 
\index{Layer@{Layer}!output\+Vals@{output\+Vals}}
\index{output\+Vals@{output\+Vals}!Layer@{Layer}}
\subsubsection{\texorpdfstring{output\+Vals}{outputVals}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
array$<$double, L\+E\+N\+\_\+\+O\+UT$>$$\ast$ \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::output\+Vals}



The values after the sums are put through the activation function. 

\mbox{\Hypertarget{class_layer_aa0a3384dec407f34bf9d93994fc6613f}\label{class_layer_aa0a3384dec407f34bf9d93994fc6613f}} 
\index{Layer@{Layer}!sums@{sums}}
\index{sums@{sums}!Layer@{Layer}}
\subsubsection{\texorpdfstring{sums}{sums}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
array$<$double, L\+E\+N\+\_\+\+O\+UT$>$$\ast$ \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::sums}



The sums after the weights are multiplied by input value`. 

\mbox{\Hypertarget{class_layer_ab4afc6b6fdda2b5fd21621f40be776fd}\label{class_layer_ab4afc6b6fdda2b5fd21621f40be776fd}} 
\index{Layer@{Layer}!weight\+Gradient@{weight\+Gradient}}
\index{weight\+Gradient@{weight\+Gradient}!Layer@{Layer}}
\subsubsection{\texorpdfstring{weight\+Gradient}{weightGradient}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
\hyperlink{class_a_f_matrix}{A\+F\+Matrix}$<$double, L\+E\+N\+\_\+\+O\+UT, L\+E\+N\+\_\+\+IN$>$$\ast$ \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::weight\+Gradient}



The weight gradients. {\ttfamily weight\+Gradient\mbox{[}i,j\mbox{]} = d(\+Error)/d(weights\mbox{[}i,j\mbox{]})}. Same shape as {\ttfamily weights}. 

\mbox{\Hypertarget{class_layer_aa362ee5edaf5b0fe8d5d7c4674ded7a1}\label{class_layer_aa362ee5edaf5b0fe8d5d7c4674ded7a1}} 
\index{Layer@{Layer}!weights@{weights}}
\index{weights@{weights}!Layer@{Layer}}
\subsubsection{\texorpdfstring{weights}{weights}}
{\footnotesize\ttfamily template$<$size\+\_\+t L\+E\+N\+\_\+\+IN, size\+\_\+t L\+E\+N\+\_\+\+O\+UT$>$ \\
\hyperlink{class_a_f_matrix}{A\+F\+Matrix}$<$double, L\+E\+N\+\_\+\+O\+UT, L\+E\+N\+\_\+\+IN$>$$\ast$ \hyperlink{class_layer}{Layer}$<$ L\+E\+N\+\_\+\+IN, L\+E\+N\+\_\+\+O\+UT $>$\+::weights}



The weights which are multiplied against the input values. This has {\ttfamily len\+Out} rows and {\ttfamily len\+In} cols. 



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
C\+:/\+Users/\+Aryan/\+C\+Lion\+Projects/\+C\+Net/src/\hyperlink{_layer_8h}{Layer.\+h}\end{DoxyCompactItemize}
